\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{mdframed}
\usepackage{bbm}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{bbm}
\usepackage[titletoc]{appendix}
\usepackage{wrapfig}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{multirow}

\def\B#1{\bm{#1}}
%\def\B#1{\mathbf{#1}}
\def\trans{\mathsf{T}}

%\renewcommand{\labelitemi}{--}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{A Computational Account of Default-Mode Function\\
by Control Theory and Reinforcement Learning}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\Prox}{Prox}
\DeclareMathOperator{\im}{im}

% some useful macros
\DeclareMathOperator{\dist}{dist} % The distance.
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\abs}{abs}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\def\Id{\mathbf{I}}
\def\1{\mathbf{1}}
\def\X{\mathbf{X}}
\def\U{\mathbf{U}}
\def\V{\mathbf{V}}
\def\v{\mathbf{v}}
\def\u{\mathbf{u}}
\def\z{\mathbf{z}}
\def\Y{\mathbf{Y}}
\def\A{\mathbf{A}}
\def\B{\mathbf{B}}
\def\C{\mathbf{C}}
\def\N{\mathbf{N}}
\def\R{\mathbf{R}}
\def\Q{\mathbf{Q}}
\def\P{\mathbf{P}}
\def\W{\mathbf{W}}
\def\K{\mathbf{K}}
\def\a{\mathbf{a}}
\def\b{\mathbf{b}}
\def\s{\mathbf{s}}
\def\x{\mathbf{x}}

\newcommand{\suggestadd}[1]{{\color{blue} #1}}
\newcommand{\suggestremove}[1]{{\color{red} \sout{#1}}}

% \nipsfinalcopy % Uncomment for camera-ready version
\nipsfinaltrue
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\author{Elvis Dohmatob, (Bert Kappen), Danilo Bzdok\\
  INRIA, Parietal team, Saclay, France\\
  CEA, Neurospin, Gif-sur-Yvette, France\\
  firstname.lastname@inria.fr}

\maketitle


\begin{abstract}
The default mode network (DMN) is the neural representation of the human
baseline mental activity.
%
Many research streams agree on its likely role in evolutionarily adaptive
envisioning of past and hypothetical scenarios to predict the environment.
The DMN would hence be dedicated to continuous autobiographical memory retrieval,
generation of hypothetical outcomes,
and reward contingency evaluation when letting the mind go.
It explains its highest energy consumption in the brain and
its intimate coupling with conscious awareness.
%
This concept paper proposes a process model that describes
\textit{how} the DMN may actually implement continuous
environmental assessment and prediction.
DMN function is recast in mathematical terms
from the perspective of control theory and
reinforcement learning.
Neurobiological evidence on the human and animal DMN
is linked to recent progress in autonomous systems research
in the machine-learning domain.
We argue that this engineering approach to DMN function can parsimoneously
explain a variety of existing neuroscientific hypotheses.
%
Formal models of the computational processes subserved in the DMN
could offer statistical control of predictive human behavior.

\end{abstract}

% official NIPS keywords
\textbf{\\keywords}: Systems Biology, mind wandering, cognitive science,
autonomous learning systems
\textbf{\\Get opinion from}: Danielle Bassett, Kai Vogeley, Mohammad Shakir,
Guillaume Dumas, Smallwood, Tenenbaum

\tableofcontents


\section{Introduction}
%
When left unperturbed, the human brain is not at rest.
Rather, the brain continues to metabolize large quantities of
oxygen and glucose energy to maintain inter-neuronal information
transfer in the absence of a focused behavioral goal (Raichle 2001;
Shulman, 1999).
This baseline energy demand is subject to surprisingly little modulations
when mentally processing environmental challenges
(Bzdok and Eickhoff, 2016a).
What has early been described as the "stream of consciousness"
in psychology (James, 1890), found its neurobiological manifestation 
in the so-called "default mode network" (DMN)
(Shulman et al., 1997). This networked collection of some of the highest regions in
the human association cortex (Mesulam et al., 1998) consistently
increased in neural activity during
unfocused everyday mind wandering (Raichle et al., 2001). That is, in
the beginning of the 21st century, brain imaging was the first technology
to allow for the discovery of a unique brain network that subserves
baseline mental activity (Bzdok et al., 2012; Bzdok et al., 2015).



The DMN appeared to be exclusive in task-induced
deactivation during various psychological experiments.
This processing of unknown information categories in the DMN has
been argued to mediate an evolutionarily conserved function for the individual.
This is all the more likely because the DMN contains the two
biggest hotspots of energy consumption in the entire central nervous system.
DMN activity also persists to a substantial degree during
sleep (Horovitz et al., 2008) and under anesthesia (Greicius et al., 2008).
Today, many authors believe that the DMN implements
probabilistic estimation of past, hypothetical, and future events. This
brain network
might have emerged to continuously predict environmental events using
mental imagery as an evolutionary advantage.
%
However, information processing in the DMN has also repeatedly
been shown to directly impact human behavior. Goal-directed task performance
improved with decreased activity in default-mode areas (Weissman et al., 2006)
and increased DMN activity was linked to more task-independent,
yet sometimes useful thoughts (Mason et al., 2007; Seli et al., 2016). 
%
Understanding of the overarching DMN function is complicated by
simultaneously controlling perception–action cycles and
maintaining baseline contemplations
across time, splace, and content domains
interfacing the external world and the self.



Consider an agent faced by choosing the next action
and being guided by rich
reenactment of really happened, hypothetically conceived, and
predicted future events to perfect ongoing behavioral performance.
A particularly attractive computational framework
to describe, quantify, and predict autonomously acting systems like the brain
can be the combination of control theory and reinforcement learning.
It is known that, the more the external world is predictable,
the more mental activity becomes detached from the actual sensory environment
and the more stimulus-independent thoughts occur
(Mason et al., 2007).
These \"offline\" processes may however contribute to optimized control of the organism.
We formalize
a policy matrix to capture the space of possible actions that can be performed
on the environment given the current state. A value matrix 
maps maps expected rewards to environmental objects and events.
Switching between states reduces to a sequential processing model.
In a model-based reinforcement learning approach,
we try to understand the environment always better
starting, at birth, with an environment that is completely unknown.
Informed by outcomes of performed action,
the DMN dynamics are constantly adapted in feed-back loops
that are calibrated by error occurrence.
A DMN framework incorporating reinforcement learning
can naturally embed human behavior
into the tension between exploitative action with immediate gains and
explorative action with longer-term reward schedules.
The implication of the DMN in a diversity of human cognitive processes
can be parsimoneously recast as minimizing prediction error
based on pervasive probabilistic simulations,
thus maximizing reward outcome at various time scales.
Such a purposeful optimization objective
may be solved by a stochastic approximation
based on a brain implementation of Markov Chain Monte Carlo sampling
(Tenenbaum2011Science).
\textit{Control} refers to the influences an agent exterts when interacting
with the environment to encourage preferred states.
Even (necessarily imperfect) memories
of past experience, random mind-wandering and dreams during sleep
may provide meanginful building blocks to iteratively improve
the predictive DMN machinery to optimize the behavior of the organism.



\section{Known Neurobiological Properties of the Default-Mode Network}
We will attempt to deconstruct the overall DMN function
by the mechanistic relevance of each individual network node
based on published evidence from neuroscience experiments.
%
The PMC and the dmPFC within the DMN
are responsible for the highest metabolic turn-over
of glucose energy consumption in humans (Raichle).
The PMC myelinates relatively late during postnatal development in monkeys
(Goldman-Rakic, 1987), generally considered to
be a sign of phylogenetic sophistication (Flechsig, 1920).
Physiological and disturbed metabolic fluctuations in the
human PMC have been repeatedly related to
phenomena of changed conscious awareness,
including anesthesia (Fiset et al., 1999),
sleep (Maquet, 2000), and forms of coma (Laureys et al., 1999).      
%
The PMC has long been speculated to reflect constant computation of
the environmental statistics and its internal representation
as an "inner minds eye" (Cavanna et al., 2006).
For instance, B\'alint's syndrome is a neurological disorder of conscious
awareness that results from damage in the bilateral parietal cortex.
Patients are plagued by an
inability to bind various individual features of the visual
environment into an integrated whole (i.e., simultanagnosia)
as well as inability to direct action towards 
current unattended environmental objects
(i.e., optic ataxia).
This can be viewed as a high-level impairment in the gathering
of information about alternative objects (i.e., exploration) as well as
leveraging these environmental opportunities (i.e., exploitation).
Congruently,
the human PMC was coupled in across two functional connectivity modalities
with the amygdala
(involved in significance evaluation) and
the nucleus accumbens (involved in reward evaluation).
Specifically, the human ventral posterior cingulate cortex was
the PMC region most connected to the laterobasal
(rather than centromedial or superficial) nuclei group
of the amygdala (Bzdok et al., 2013b).
This amygdalar subregion has been proposed to
continuously scan environmental input
for biological significance assessment
(Adolphs, 2010; Aggleton et al., 1980; Bzdok et al., 2011;
Ghods-Sharifi et al., 2009; LeDoux, 2007).
%
Indeed,
electrophysiological recordings in animals implicated the PMC in
strategic selection (Pearson et al., 2009), risk assessment (McCoy and Platt, 2005),
and outcome-contingent behavioral modulation (Hayden et al., 2008),
while its retrosplenial portion was
more specifically implicated in approach-avoidance behavior (Vann et al., 2009).
Neuron spiking activity in the PMC allowed distinguishing
whether a monkey would persue an exploratory or exploitative
behavioral strategy in a complex food foraging task (Pearson2009).
Single-cell recordings in the monkey PMC
demonstrated this brain region's sensitivity to
subjective target utility (McCoy and Platt, 2005) and integration
across individual decision-making instances (Pearson et al., 2009).
This DMN node encoded the
preference or aversion to options with uncertain reward outcomes
and its spiking activity was more associated with
subjectively perceived relevance of a chosen object
than by its factual value, based on an internal currency of value.
In fact, direct stimulation of PMC neurons
promoted exploratory action towards options
with unsafe reward outcomes that were previously shunned.
Graded changes in firing rates of PMC neurons
indicated changing choices in upcoming trials and their neural patterns were
distinct from spike firings that indicated choosing either option.
Also in humans, 
the DMN has been shown to gather and integrate information
over different paragraphs in an fMRI study on auditory narratives (Simony 2016).
%
The retrosplenial portion of the PMC can subserve action possibilities
and evaluation of reward contingencies by integrating these with
information from memory and altered perspective frames.
Regarding memory retrieval, retrosplenial lesions have been
consistently associated with anterograde and retrograde memory impairments
of various kinds of sensory information
in rabbits (Gabriel and Talk, 2001) and humans
(Rudge and Warrington, 1991; Valenstein et al., 1987).
Regarding perspective frames, this PMC subregion has been
proposed to mediate between the organism's egocentric
(i.e., focused on sensory input) and
allocentric (i.e., focused on world knowledge) viewpoints
in animals and humans
(Burgess, 2008; Epstein, 2008; Valiquette and McNamara, 2007).
%
The PCC may consequently monitor the subjective outcomes
of possible decisions and integrates that information
with memory, perspective frames, and
reward schedules into higher-level strategies. 
Perceived value that differs across individuals updates
statistical assessment of the environment
to predict delayed reward opportunities in the future.
In doing so, the PMC continuously adapts to changes
in both the external environment and internal milieu
that modulate strategic behavioral adjustment in volatile environments.



The dmPFC subserves predominantly ambiguous amodal processes
across time, space, and content domains in
sensory-independent top-down pathways.
This DMN node has been described as a “mental sketchpad” (Goldman-Rakic, 1996),
potentially implicated in de-novo generation and binding
of meaning representations instructed by stored semantics and memories.
%
The dmPFC may thus enable inference, representation, and assessment
of one's own and other individuals' action and thoughts.
Patients with neurological lesions in the prefrontal cortex
are known to adapt to novel situations and stimuli with difficulty
(Stuss, D. T. \& Benson, D. F. The Frontal Lobes, Raven, New York, 1986).
Specifically, neural activity in human dmPFC
reflected expectations about other peoples actions and errors thereof
(Suzuki et al., 2012; van Schie et al., 2004).
dmPFC activity indeed explained the proficiency decline
of inferring other peoples thoughts in the elderly (Moran et al., 2012).
Some dmPFC neurons in macaque monkeys exhibited a preference
for processing others', rather than own, behavior
with fine-grained adjustment of contextual circumstances (Yoshida et al., 2011).
Also the topographically neighboring dorsal anterior cingulate cortex
has been linked to computing values and efforts of
persisting a behavioral plan versus switching the
environmental context in several lesion studies (Rushworth NN 2016).
%
Such highly abstract neural computations necessarily rely on the
generation of probabilistic internal information drawing from
episodic memory retrieval, generative construction processes,
and explicit knowledge of the external world.
%
According to computational neuroimaging experiments,
the dmPFC activity preferentially models stimulus-value associations of
possible actions that are not actually executed,
whereas the vmPFC activity models value in the environment that leads performed actions
{Nicolle, 2012 \#3750}.



The vmPFC subserves less ambiguous subjective-value-related evaluative processes
reward-informed and risk estimation of self-relevant environmental stimuli.
This DMN node is more closely associated with
orchestrating adapted behavior by bottom-up-driven
processing of “what matters now”,
probably drawing on model-based value representations
\cite{doherty2015structure}.
Quantative lesion findings across 344 human individuals confirmed
a gross impairment in value-based decision making
(Glaescher et al., 2016).
The vmPFC is preferentially connected with limbic and reward-related areas.
The vmPFC has been observed to have monosynaptical connections
with the nucleus accumbens
in axonal tracing studies in monkeys (Haber et al., 1995; Ferry et al., 2000).
The NAc is thought to be linked to reward mechanisms that not
only modulate motivated behavior towards basic survival needs but also
subserve reinforcement learning in humans more broadly
(O'Doherty 2015).
This is consistent with diffusion MRI (dMRI) tractography in humans and monkeys
(Croxson et al., 2005) that
quantified this brain regions to
be substantially more likely connected to the vmPFC than dmPFC in both species.
Two functional connectivity modalities in humans strongly connected
the vmPFC with the nucleus accumbens, hippocampus (HC),
and posteriomedial cortex.
%
The vmPFC is often proposed to be involved in (external) emotional
reactions and own (visceral) arousal.
Real or imaginged bodily states could be mapped in the vmPFC
as a bioregulatory disposition governing cognition
and decision making (Damasio, 1996; Nauta, 1971)
In neuroeconomic studies of human decision making,
the vmPFC consistently reflects an individual’s subjective
valuation (Behrens et al., 2008; Boorman et al., 2009; Knutson et al., 2005; Plassmann et al., 2007)
This may be why performance within and across participants
was related to state encoding in OFC.
Such a “cognitive map” of action space was argued to encode
the current task state even when states are unobservable from sensory input,
which was shown to be critical for behavior.
The GMV of the vmPFC and nucleus accumbens
correlated with indices of social reward attitudes and
value-guided behavior (Lebreton et al., 2009).
Additionally,
independent whole-brain analyses from structural
neuroimaging studies related the gray-matter volume GMV of the vmPFC,
more consistently than any other
brain region, to indices of
social competence and social network complexity,
among the most complicated decision that humans and monkeys take
(Lebreton et al., 2009; Powell et al., 2010; Lewis et al., 2011; Sallet et al., 2011).
%





The HC is well known to be involved in memory and
spatial navigation in animals and humans
(von Bechterew, 1900; Scoville and Milner, 1957; O’Keefe and Dostrovsky, 1971; Maguire et al., 2000).
Its highly recursive anatomical architecture
may be specifically designed to allow reconstructing
entire episodes of experience from memory fragments.
%
While the HC in the medial temporal lobe system
is traditionally believed to allow remembering the past,
there is now increasing evidence for a role
in constructing mental models in general (Schachter/Buckner;
Maguire 2016).
Indeed,
hippocampal damage is
not only associated with an impairment in reexperiencing the past (i.e., amnesia),
but also thinking about one’s own future and
imagining new fictitious experiences more broadly (Hassabis/PNAS).
Mental scenes created by hippocampus-lesioned patients exposed a lack of
spatial integrity, richness in detail, and overall coherence.
%
Single-cell recordings in the animal hippocampus revealed
some constantly active neuronal ensembles whose firing coincided with
distinct locations in space while the animal navigated through its surroundings.
London taxi drivers, individuals with high performance in spatial navigation,
were shown to exhibit increased grey matter volume in the
posterior hippocampus (Maguire et al. 2006b).
But encoding and generative reconstruction in the hippocampus extends
beyond mere spatial knowledge of the environment.
Based on large-scale recordings of hippocampal neuronal populations,
complex spiking patterns can be followed across extended periods including
their modification of input-free self-generated patterns
after environmental events (buszaki2004 theta oscillations).
Specific spiking sequences, that were elicited in experimental task conditions,
have been shown to be reenacted spontaneously during sleep.
Moreover, spike sequences measured in hippocampal place cells of rats
featured reoccurrence directly after experimental trials
as well as directly before upcoming experimental trials (Diba + Buzsaki 2007).
Such hippocampal ensemble burts during rest and sleep
have been proposed to be critical in communicating local information
to the neocortex for long-term storage, potentially also in the DMN.
These mechanisms of the hippocampus probably make important contributions to the
recollection of autobiographical memory episodes and other
reexperienced or newly generated mental scenarios (Hassabias2007).
%
The HC thus orchestrates elements of experienced environmental aspects for
consolidations based on reenactment and for integration into 
rich mental scene construction. In this view, the HC my even influence
ongoing perception in the current environment (Maguire 2016).
-> self-play in deepmind Go
-> Kai had an interesting idea: the deja-vu effect could be when you already accidentally perturbed past such that it really occures in reality later


Finally,
the right and left TPJ are known to have hemispheric differences
according to their cytoarchitectonic borders and gyrification pattern
(Caspers et al., 2008; Caspers et al., 2006).
Neuroscientific investigations on hemispheric functional specialization
have converged to the right versus left cerebral hemisphere as dominant for
attention versus language functions (Broca 1865, Wernicke 1874, Lichtheim 1885;
Gazzaniga, Bogen et al. 1965, Sperry 1982, Stephan, Marshall et al. 2003).
%
The aRTPJ is a key node network is central for
action initiation during externally structured tasks and
sensorimotor control by integrating supramodal stimulus-guided attention
(Corbetta et al., 2002).
Involvement of this DMN node was repeatedly reported in
multi-step action execution \cite{hartmann2005takes},
visuo-proprioceptive conflict (Balslev et al., 2005) and
multi-modal detection of sensory changes across
visual, auditory, or tactile stimulation in a multi-modal fMRI study
(Downar et al., 2000).
In humans, direct electrical stimulation of the
RTPJ during neurosurgery was associated with altered perception
and stimulus awareness (Blanke et al., 2002).
%
Importantly, the RTPJ has been shown to be intimately related to
supramodal prediction and error signaling.
It was argued that the RTPJ encodes actions and ensuing outcomes
without necessarily relating those to outcome value
\cite{liljeholm2013neural, hamilton2008action,
jakobs2009effects}.
Neural activity in the RTPJ has been argued to be responsible 
for stimulus-driven attentional reallocation to 
salient and surprising sources of information
as a ‘‘circuit breaker’’ that recalibrates control and maintenance systems
(Bzdok et al., 2013; Corbetta et al., 2008).
Indeed, patients with right TPJ damage have particular difficulties
with multistep actions (Hartmann et al. 2005).
In the face of large discrepancies between actual and previously predicted
environmental events the RTPJ acts a potential switch between
externally-oriented mind sets focussed on the
sensory world and internally-oriented mind sets focussed
on self-relevant mind-wandering of possible mental scenarios.
Transient RTPJ in humans for instance disrupted the
impact of predicted intentions of other individuals (Young et al. 2010b),
a capacity believed to be subserved by the DMN.
The RTPJ might hence be an important player that shifts away
from the default active ‘‘internally directed’’ processes
to deal instead with immediate environmental objects and contexts.
-> (slow) endogeneous changes in state transitions by the policy and value matrices
For example, the alarm at the museum is not part of the task set of listening to the guide’s discussion of Hieronymous Bosch, but it is clearly a behaviourally relevant stimulus. <=> MCMC in HC / PCC


The left TPJ in turn exhibits a close topographical relationship to
Wernicke's area
involved in the comprehension or understanding of written and spoken language.
Neurological patients with damage caused to Wernicke's area
have a major impairment of language comprehension
when listening to others or reading a book.
Their speech
preserves natural rhythm and about normal syntax, yet the
voiced sentences are devoid of meaning (i.e., aphasia).
Abstracting from the typical semantic interpretations in linguistics
and neuropsychology,
the LTPJ probably mediates access to and integrating of world knowlege.
For instance, LTPJ lesions also entail problems in recognizing
others' pantomimed action towards objects
without obvious relation to processing any language content
(Varney and Damasio 1987; Rothi et al. 1991; Buxbaum et al. 2005).
%
Also inner speech hinges on knowledge retrieval of statistical structure
about the physical and inter-personal world.
Multivariate volumetric analyses showed that the internal production of
formulated thought ("language of the mind") is closely related to the left TPJ
(Geva et al., 2011).
A number of neuroimaging studies demonstrated LTPJ activity responses
to sementically aberrant words in sentences, which suggests a role
in binding a given semantic concept into an integrated whole.
Further,
episodic memory recall and imagination strongly draw on
complex world knowledge.
Isolated building blocks of world statistics probably get reassembled
in internally generated visual scenarios that
navigate present action, weigh hypothetical possibilities, and forcast the future.
%
The LTPJ may hence facilitate the automated prediction of events
by incorporating experience-derived models of the world
into ongoing action, planning, and problem solving.
world -> policy matrix driven computation
self -> value matrix driven computation


\section{Methods}

\subsection{Linear control network model for brain organization}
Consider the following linear time-invariant (LTI) dynamical system as a (toy) model for high-level brain function
\begin{equation}
  \dot{\x}(t) = \A\x(t) + \B\u(t).
  \label{eq:lti}
\end{equation}
Here, the $n$-by-$n$ matrix $\A$ denotes a model of the brain's wiring (for example a resting state connectome computed from an anatomical atlas), while the $n$-by-$k$ ``input matrix'' $\B$ describes which nodes can be controlled by us, an external \textit{controller}, via medical intervention or a careful choice of stimulus presentation, for example.
At time $t \ge 0$, let  the $n$-vector  $\x(t) := (\x_1(t),\ldots,\x_n(t))$ encodes the state of the network (one value for each node). The aim is to supply values of $\u_1(t),\ldots,\u_k(t)$ for $k \le n$ controls as a function of the time $t$,  to take the system from any prescribed initial state $\x(0) = \x^{\text{init}} \in \mathbb R^n$ to any prescribed final state $\x(\tau) = \x^{\text{fin}} \in \mathbb R^n$ in finite time $\tau < \infty$.
When such a controlling is possible, we say that the system $(\A,\B)$ is \textit{controllable}. A precise sufficient and necessary condition for such controllability is the Kalman condition: $(\A,\B)$ is controllable iff the $n \times nk$ \textit{controllability matrix}
\begin{equation}
  \C := (\B|\A\B|\ldots|\A^{n-1}\B)
\end{equation}
has full rank, i.e
\begin{equation}
  rank(\C) = n.
\end{equation}

\paragraph{How many controls do we need at best ?}
In the thermodynamic limit ($n \rightarrow \infty$), statistical physics \cite{Liu2011} gives the extremely good estimate
\begin{equation}
  n_0(\langle k\rangle, \gamma) \approx \exp\left(-\frac{1}{2}\left(\frac{\gamma-2}{\gamma - 1}\right)\langle k \rangle\right),
\end{equation}
where $\gamma \ge 1$ is the \textit{scale-free} parameter for the node-degree distribution of the network described by $\A$, and $\langle k \rangle$ is the mean degree. Letting $\gamma \rightarrow \infty$, one recovers the Erdos-Renyi value
\begin{equation}
  n_0(\langle k\rangle) = \exp\left(-\frac{1}{2}\langle k\rangle\right),
\end{equation}
which agrees perfectly with its known analytic value. One identifies two radically different regimes:
\begin{itemize}
  \item the manageable regime $\gamma > \gamma_c := 2$, in which $n_0 < 1$, and so we only need fewer controls than total number of nodes, and
\item the unmanageable regime ``$\gamma \le \gamma_c$'', in which  $n_0 = 1$, where each node must be controlled explicitly.
\end{itemize}

\paragraph{Why linear dynamics ?}
As explained in \cite{Liu2011}, the choice of linear dynamics over a more general nonlinear dynamics can be justified as follows:
\begin{itemize}
  \item  Conclusions drawn from linear dynamics can be
extended to nonlinear systems.
\item If the controllability matrix of the linearized system
has full rank at all points, then it is sufficient for most
systems to say that the actual nonlinear system is
controllable (i.e. small signal model).
\end{itemize}

\paragraph{A note on stability.}
For stability in the LTI model \eqref{eq:lti} and hence in the constrained path integral \eqref{eq:hj} below, a sufficient (and necessary ?) condition is that all eigenvalues of the $A$ have negative real-part. One way to impose this is to add self-loops with a small negative weight.
\paragraph{Meta brain.}

\subsection{Optimal control: an LQR feedback controller}
We propose to use a linear quadratic regulator (LQR) for the feedback controller. Thus, consider the time-varying \text{value function} $V: [0, \tau] \times \mathbb R^n \rightarrow \mathbb R$, defined by the following Hamilton-Jacobi cost-to-go
\begin{equation}
  \begin{split}
    &V(t, \z) := \min_{\u} \frac{1}{2}\x_{\text{fin}}^T\Q_{\text{fin}}\x_{\text{fin}} + \int_{0}^\tau\left(\frac{1}{2}\x(t)^T\Q\x(t) + \frac{1}{2}\u(t)^T\R\u(t)\right)dt,\\
    &\text{subject to } \x(t) = \z, \dot{\x}(t) = \A\x(t) + \B\u(t),
  \end{split}
  \label{eq:hj}
\end{equation}
where the matrices $\Q$ and $\R$, precised by design considerations and subject to meta-optimization, are restricted to be positive semi-definite and positive definite, respectively. A classical calculation (reminiscent of the \textit{Pontryagin minimization principle}) reveals that the optimal feedback control in \eqref{eq:hj} is given by
\begin{equation}
  \u(t) = -\K(t)\x(t),
\end{equation}
where
\begin{equation}
  \K(t) := \R^{-1}\B^T\P(t)\text{ (Kalman gain matrix)}
\end{equation}
and the time-varying positive semi-definite matrix $\P(t)$ solves following differential Riccati equation (DRE)
\begin{equation}
\begin{split}
&-\dot{\P}(t) = \A^T\P(t) + \P(t)\A - \P(t)\B\R^{-1}\B^T\P(t) + \Q\\
&\text{subject to }\P(\tau) = \Q_{\text{fin}}.
\end{split}
\end{equation}
%% \subsection{Minimal control energy controller}

\subsection{Demystifying the free-energy principle and active-inference}
The so-called free-energy principle in its present form (including notions like ``generative density'', ``recognition density'', etc.) can be traced back to works of Dayan \& Hinton \cite{dayan1995helmholtz} in which they introduced the so-called \textit{Helmholtz machine}, a hierarchical factorial directional deep belief-net (DBN).

In this section, we will develop from first-principles, the bare-bones minimalistic ideas needed to build a free-energy principle for general decision-making. This ideas were first developed by Hinton et al. in the early 90s in building their Helmholtz machine. Theories like Friston's free-energy principle and active-inference will then emerge as particular instances of this general framework, with particular design choises. For example the Friston theory axiomatizes that the brain uses a (problematic, as it implicitly assumes that posterior of each hidden unit is factorial, e.g) wake-sleep algorithm to train the underlying Helmholtz machine, etc.

\subsubsection{Helmholtz free-energy and the generative model}
\begin{table}[H]
  \begin{tabular}{p{2cm}|p{11cm}}
         \hline
         \textbf{symbol}    & \textbf{description}  \\ \hline
         $\langle X\rangle_p$ & Expectation (a.k.a average, a.k.a mean value) of the
         random quantity $X$ w.r.t to the probability density $p$, formally defined by $\langle E\rangle_p := \sum_{z}p(z)X(z)$.\\ \hline
         $\mathcal H(p)$ & Information-theoretic entropy of a probability density $p$, formally defined by $\mathcal H(p) := -\sum_{z}p(z)\log(p(z))$,
          with the usual convention $0 \log(0) := 0$.\\ \hline
         $D_{KL}(q||p)$ & The Kullback-Leibler divergence between the probability densities $q$ and $p$ respectively, formally defined by $D_{KL}(q||p) := \sum_{z}q(z)\log(q(z)/p(z))$.\\ \hline
             $\zeta$ & Observations. In Friston's free-energy principle this has a decomposition in to two terms: the brain's internal state $b$ and sensory inputs $s$, i.e $\zeta = (s, b).$ \\ \hline
             $\psi$ & Hidden variables. This should be understood as the unobservable states of the external environment (to which the brain is trying to adapt by learning).\\ \hline
             $p_G(.|\zeta)$ & Generative density for ...\\ \hline
         $p_R(.|\zeta)$ & Recognition density for ... Does some kind of predictive coding (?).\\ \hline
         $F_G(\zeta)$ & Helmholtz free-energy for a model $p_G$ of generating the observation $\zeta$. This measures the surprise incured upon observing $\zeta$ generated by the model $G$.\\ \hline
         $F^R_G(\zeta)$ & Variational Helmholtz free-energy from model $G$
          to $R$.  Note that $F^G_G = F_G$.\\ \hline
  \end{tabular}
  \caption{Table of notations.}
\end{table}
Our starting point will be to build an approximation $p_G$ for the true density $p$ of the observations, so that this approximate density corresponds to the partition function of thermodynamic system. So,
\begin{eqnarray}
  \begin{split}
    \text{generative surprise } &= -\log(p_G(\zeta)) = -\log(p_G(\zeta)) \times 1 = -\log(P_G(\zeta))\sum_{\psi}p_G(\psi |\zeta)\\
    &= -\sum_{\psi}p_G(\psi, \zeta)\log(p_G(\zeta))
    =-\sum_{\psi}p_G(\psi |\zeta)\log(p_G(\psi, \zeta)/p_G(\psi|\zeta))\\
    &= \sum_{\psi}p_G(\psi |\zeta)\log(p_G(\psi|\zeta))-\sum_{\psi}p_G(\psi |\zeta)\log  (p_G(\psi, \zeta))\\
    &= -\langle \log  (p_G(., \zeta)) \rangle_{p_G(. |\zeta)} - \mathcal H(p_G(. |\zeta))\\
    &= \langle E_G(., \zeta) \rangle_{p_G(. |\zeta)} - \mathcal H(p_G(. |\zeta)) := F_G(\zeta)
  \end{split}
  \label{eq:helm}
\end{eqnarray}
where $E_G(\psi, \zeta)$ is the energy at \textit{macrostate} $\psi$ of a fictive thermodynamic system defined by setting
\begin{equation}
  E_G(\psi, \zeta) := -\log(p_G(\psi, \zeta)).
  \label{eq:gibbs}
\end{equation}
%% In the above, the variable $\psi$ is a hidden variable, and can / should be interpreted as the unobservable state of the external world.
The last quantity in \eqref{eq:helm} defined formally as
\begin{equation}
  F_G(\zeta) := \langle E_G(., \zeta) \rangle_{p_G(. |\zeta)} - \mathcal H(p_G(. |\zeta))
\end{equation}
is the \textit{Helmholtz free-energy} (at unit temperature!). %% , modelled as the physical system for which $p_G(\psi|\zeta)$ dictates the occupation probabilities of macrostates $\psi$
Thus generative surprise and generative Helmholtz free-energy are different views on exactly the same object.

The goal of the brain is then to optimize over the generative model $G$: to iteratively or analytically modify the generative density $p_G(.|s)$, so as to minimize surprise. It turns out that a direct attempt to attack this optimization problem by gradient descent on the free-energy $F_G(s)$ is furtile: the parameter update steps are not ``very clean'', and require rather cumbersome and heavy computations. A workaround is then to introduce a second density $p_R(.|\zeta)$ called a \textit{recognition} density to work in tandem with the generative density $p_G(.|\zeta)$, as a trick for doing approximate inference. The former dreams / fantacizes whilst the latter tries to generate sensations which match these dreams! This primal-dual idea, first proposed in Hinton et al. 1995, is at the heart of the general free-energy principle that we'll introduce shortly.

\subsubsection{Variational Helmholtz free-energy and the bottom-up recognition sub-model}
In this subsection, we'll present an insightful upper bound for the generative surprise (i.e generate Helmholtz free-energy), called the \textit{variational} (Helmholtz) free-energy. As an avant-gout of what is to come shorty, let's just note that the well-known \textit{free-energy principle} is simply a workaround whereby the minimization surprise (intractable) is replaced with the  minimization a carefully chosen upper bound thereof.

Invoking \eqref{eq:gibbs} and applying Bayes rule, we get the Gibbs
distribution
\begin{equation}
  p_G(\psi|\zeta) = \frac{p_G(\psi|\zeta)}{p_G(\zeta)} = \frac{\exp(-E_G(\psi, \zeta))}{Z_G(\zeta)} = \frac{\exp(-E_G(\psi, \zeta))}{Z_G(\zeta)},
\end{equation}
where $Z_G(\zeta) := \log(p_G(\zeta)) = \sum_{\psi'}\exp(-E_G(\psi',\zeta))$, the normalizing \textit{partition function} for the model \ref{eq:gibbs}.
Whence $\forall \psi$, $p_G(\zeta) = Z_G(\zeta) = \exp(-E_G(\psi, \zeta)) / p_G(\psi|\zeta)$, and so we have the invariance relation
\begin{equation}
F_G(\zeta)\overset{\eqref{eq:helm}}{=} -\log(p_G(\zeta)) = -\log(Z_G(\zeta)) = E_G(\psi, \zeta) + \log(p_G(\psi|\zeta)).
  \end{equation}
Now, in the equation above, the LHS only depends on the generative model $G$ and the data point $\zeta$: it doesn't depend on the hidden variable $\psi$, etc. So, taking expectations w.r.t an arbitrary density\footnote{The conditioning in $P_R(.|\zeta)$ is because this density is selected from a world in which the sensory inputs and internal brain state vector $\zeta$ is assumed already observed.} $p_R(.|\zeta)$ yields
\begin{equation}
  \begin{split}
    F_G(\zeta) &= -\log(Z_G(\zeta)) = \langle E_G(., \zeta)\rangle_{P_R(.|\zeta)} + \sum_{\psi}p_R(\psi|\zeta)\log(p_G(\psi|\zeta))\\
    &= \langle E_G(., \zeta)\rangle_{P_R(.|\zeta)} - \mathcal H(p_R(.|\zeta)) - \sum_{\psi}p_R(\psi|\zeta)\log(p_R(\psi|\zeta)/p_G(\psi|\zeta))\\
    &= F^R_G(\zeta) - D_{KL}(P_R(.|\zeta) || P_G(.|\zeta)),
  \end{split}
  \label{eq:fe}
\end{equation}
where $F^R_G(\zeta)$ is the \textit{variational} Helmholtz free-energy from $R$ to $G$ defined by
\begin{equation}
  F^R_G(\zeta) := \langle E_G(., \zeta)\rangle_{P_R(.|\zeta)} - \mathcal H(p_R(.|\zeta))
\end{equation}
and $D_{KL}(P_R(.|\zeta) || P_G(.|\zeta))$ is the Kullback-Leibler divergence between the $p_R(.|\zeta)$ and the generative density $p_G(.|\zeta)$. Note that $F^G_G = F_G$.

\subsubsection{A general free-energy principle}
We can resume the situation as follows:
\begin{mdframed}
\begin{equation}
  \begin{split}
    \text{generative surprise } &:= -\log(p_G(\zeta)) = F_G(\zeta) \\
    &=\underbrace{F^R_G(\zeta)}_{\text{accuracy}} - \underbrace{D_{KL}(P_R(.|\zeta) || P_G(.|\zeta))}_{\text{complexity}} \\
    &\le F^R_G(\zeta),
    \text{ with equality if }p_R(.|\zeta) = p_G(.|\zeta),
    \end{split}
\end{equation}
\end{mdframed}
where we have used the fact that KL divergence is always nonnegative.
%% In fact, we have the following theorem
%% \begin{theorem}
%%   It holds that
%%   $$
%%   \text{Minimal generative surprise = minimal (Helmholtz) variational free-energy},$$
%%   i.e
%%   \begin{equation}
%%     \min_{G}F_G(\zeta) = \min_{G,R}F^R_G(\zeta)
%%     \end{equation}
%% \end{theorem}

\subsubsection{Helmholtz machines and the wake-sleep algorithm}
% \begin{mdframed}
  \textbf{Assumption:} In both generative and recognition components of the network, there is conditional independence of  neurons in the same layer, given the data (i.e input from lower more primitive layers). Precisely
$$
p_G(\psi^{(l)}|\zeta) = \Pi_{k=1}^{h_l}p_G(\psi_k^{(l)} | \zeta),\hspace{1cm}
p_R(\psi^{(l)}|\zeta) = \Pi_{k=1}^{h_l}p_R(\psi_k^{(l)} |\zeta)
$$
% \end{mdframed}

\subsubsection{Friston's active-inference}
This is nothing but an application of the Dayan's wake-sleep algorithm for training a Helmholtz machine model of the brain...

The following critics can be made:
\begin{itemize}
  \item 
    As noted by Dayan et al. (\textit{Variants of Helmholtz machines}), the inter-neuronal intra-layer independence assumption which is at the center of the HM becomes severely problematic as it is agnostic to the known organization of cortical layers...
  \item A drawback of the wake-sleep algorithm is that it requires a concurrent models (generative and recognitiion), which together do not correspond to optimization of (a bound of) the marginal likelihood (because of the incorrect KL used therein, etc.).
  \item Also, note that the wake-sleep algorithm doesn't do backprop! This is due to
    technical difficulty in getting derivatives of loss function w.r.t
    recognition weights $\W^R$).
  \item This difficulty was removed in the 2010s by Kingma et al., an other groups, via a ``reparametrization trick''.
    \end{itemize}


\subsection{Free-energy minimization via variational auto-encoders}
Following Kingma et al. 2014 \cite{kingma2013auto}, define the data-dependent auxiliary random function
\begin{eqnarray}
  f_{G,R}(., \zeta) :\psi \mapsto \log(p_G(\psi,\zeta)) - \log(p_R(\psi|\zeta)).
\end{eqnarray}
Then we can rewrite the
variational free-energy as
\begin{eqnarray*}
  \begin{split}
    F_G^R(\zeta) &:= \langle E_G(., \zeta) \rangle_{p_R(. |\zeta)} - \mathcal H(p_R(. |\zeta)) = \langle E_G(., \zeta) + \log(p_R(.|\zeta))\rangle_{P_R(.|\zeta)}\\
    &=\langle -\log(p_G(.,\zeta)) + \log(p_R(.|\zeta))\rangle_{P_R(.|\zeta)}\\
    &= -\langle f_{G,R}\rangle_{P_R(.|\zeta)} \approx -\frac{1}{M}\sum_{m=1}^Mf_{G,R}(\psi^{(m)}), \text{ with }\psi^{(1)},\ldots,\psi^{(M)} \sim p_R(.|\zeta), \text{ and }M \rightarrow \infty.
    \end{split}
\end{eqnarray*}

\begin{mdframed}
  \textbf{Problem:} How do we sample from the recognition density $p_R(.|\zeta)$ in such a way that the sampling process is differentiable w.r.t the weights of the recognition network $\W^R$ ?
\end{mdframed}
\paragraph{Solution: The reparametrization trick.}
\begin{itemize}
\item Choose $\epsilon \sim p_{\text{noise}}$ (noise distribution, independent of $\W^R$!)
\item Set $\phi = g(\W^{R}, \zeta, \epsilon)$, where $g$ is an appropriate class $\mathcal C^1$ function
  \begin{itemize}
  \item results in a sample $\phi \sim p_R(.|\zeta)$, from the correct
    posterior
    \end{itemize}
\end{itemize}

\begin{table}[H]
  \begin{tabular}{p{1.7cm}|p{1.5cm}|p{1.9cm}|p{2.1cm}|p{5cm}}
         \hline
         Posterior & $p_R(.|\zeta)$ & noise & $g(\W^R,\zeta,\epsilon)$ & Also \\ \hline
         Normal & $\mathcal N(\mu,\sigma)$ & $\epsilon \sim \mathcal N(0, 1)$ & $\mu + \sigma \odot \epsilon$ & Location-scale family: Laplace, Elliptical,
         Student’s t, Logistic, Uniform, Triangular, ... \\ \hline
         Exponential & $\mathcal \exp(\lambda)$ & $\epsilon \sim \mathcal U([0, 1))$ & $-\log(1-\epsilon)/\lambda$ & Invertible CDF: Cauchy, Logistic, Rayleigh, Pareta, Weibull, Reciprocal, Gompert, Gumbel, Erlan, ... \\ \hline
         Other & $\log\mathcal N(\mu,\sigma)$ & $\epsilon \sim \mathcal N(0, 1)$ & $\exp(\mu + \sigma \odot \epsilon)$ & Gamma, Dirichlet, Beta, Chi-squared, F, ... \\ \hline
  \end{tabular}
  \caption{Reparametrization trick for a variety of models. Kingma et al. 2014 \cite{kingma2013auto}.}
\end{table}

The result is a scheme for training DBNs via good-old backprop!

%% For fixed $p_G(.|\zeta)$, the optimal choice for $p_R(.|\zeta)$ is given analytically by
%% \begin{equation}
%%   p_R(\psi|\zeta) \propto p_G(\psi|\zeta)\exp(-\Delta E_{G \rightarrow R}(\psi, \zeta)).
%% \end{equation}

%% \subsection{A thermodynamic model for bounded rationality, aka robust
%%   optimality}
%% \begin{itemize}
%%   \item Recall that an agent is said to have \textit{bounded rationality} if they must take into account the cost of finding solutions to problems, and not just the utility of the final state.
%%   \item For example, consider an agent that must operate under a limited lifetime and/or computation cost.
%%   \item This is in contrast to agents with \textit{unbounded rationality} considered in classical game theory.
%%     \end{itemize}
%% The material presented is a revisit of \cite{braun2011path}. See also \cite{ortega2013thermodynamics}
%% \paragraph{Utility functions and conjugate pairs.}
%%   Let $(\Omega, \mathcal F, P)$ be a probablity space. A function $U: \mathcal F \rightarrow \mathbb R$ is said to be a \textit{utility function} for this space if the conditional utility $U(A|B) := U(A \cap B) - U(B)$ has the following propertites:
%%   \begin{itemize}
%%   \item additivity: $U(A_1 \cap A_2 | B) = U(A_1|B) + U(A_2|B)$, for all events $A_1, A_2, B \in \mathcal F$.    
%%   \item statistic: there exists a function $f_{U} :\mathbb R_+ \rightarrow \mathbb R$ such that $U(A|B) = f(P(A|B))$, for all events
%%     $A,B \in \mathcal F$.
%%   \item monotonicity: $f_{U}$ is strictly increasing.
%%     \end{itemize}

%% \begin{theorem}
%%   The only functions $f: \mathbb R_+ \rightarrow \mathbb R$ which is such that $U(A|B) \equiv f(P(A|B))$ any probability space $(\Omega, \mathcal F, P)$ and utility function $U$ thereupon are of the form
%%   \begin{equation}
%%     f = \alpha \log(.),
%%     \label{eq:boltzmann}
%%   \end{equation}
%%   where $\alpha > 0$.
%% \end{theorem}

%% \textbf{XXX: equation \eqref{eq:boltzmann} above looks like Boltzmann's formula (on his gravestone...)!}

%% Such $U$ and $P$ are said to form a \textit{conjugate pair} at temperature $\alpha$.


%% \paragraph{Example.}
%% Given a utility function $U$ on a probability space $(\Omega, \mathcal F, *)$, the \textit{Gibbs measure} at temperature $\alpha > 0$ and energy levels $(-U(\omega))_{\omega \in \Omega}$ is defined to be the probability measure (on thesame measurable space)
%%   \begin{equation}
%%     P(\omega) = \frac{1}{Z_{U}(\alpha)}\exp\left(\frac{1}{\alpha}U(\omega)\right), \; \forall \omega \in \Omega,
%%   \end{equation}
%%   where

%%   \begin{equation}
%%     Z_{U}(\alpha) := \sum_{\omega \in \Omega}\exp\left(\frac{1}{\alpha}U(\omega)\right)
%%   \end{equation}
%%   is a normalization constant called the \textit{partition function} of $U$.
%%   It's not hard to see that $U$ and the $P$ above form a conjugate pair.

%%   \paragraph{Free-utility functional.}
%%     Let $(U, P)$ be a conjugate pair at temperature $\alpha > 0$  on a measurable space $(\Omega, \mathcal F)$. Given another probability measure $P'$ on the same space, define it's \textit{free utility} as
%%     \begin{equation}
%%       J(P'|U, P) = \langle U \rangle_{P'} + \alpha \mathcal H(P'),
%%     \label{eq:free_u}
%%     \end{equation}
%%     where
%%     \begin{equation}
%%       \mathcal H(P') := \langle \log(P') \rangle_{P'} := -\sum_{\omega \in \Omega}P'(\omega)\log(P'(\omega))
%%     \end{equation}
%%     is the \textit{entropy} of $P'$ (measured in the Naperian base $e \approx 2.73$). It's not difficult to establish the upper bound
%%   \begin{equation}
%%     J(P'|U) \le J(P|U) = \sum_{\omega \in \Omega}U(\omega) =: U(\Omega).
%%   \end{equation}
%%   In particular, if $P$ is the Gibbs measure at temperature $\alpha$ corresponding to $U$, then the upper bound above reduces to the \textit{log-partition function}
%%   \begin{equation}
%%     J(P'|U) \le U(\Omega) = -\alpha \log(Z_{U}(\alpha)).
%%     \end{equation}

%%   \paragraph{The free-energy / utility principle (of Friston ?).}
%%   We are now in shape to introduce the notion of free-energy for model transitions, and a variational principle for optimizing it. Consider thus an initial system described by a conjugate pair $(U_{\text{ini}}, P_{\text{ini}})$ at temperature $\alpha > 0$. We want to transform this to a new model by adding constraints represented by the utility function $\Delta U$.  The resulting system has final utility $U_{\text{fin}} = U_{\text{ini}} + \Delta U$. The difference in free-utility is then
%%   \begin{equation}
%%     \Delta J_{(U_{\text{ini}}, P_{\text{ini}}) \rightarrow P_{\text{fin}}} := J_{\text{fin}} - J_{\text{ini}} = \underbrace{\langle \Delta U\rangle_{P_{\text{fin}}}}_{\textbf{accuracy}} -  \underbrace{\alpha D_{\text{KL}}(P_{\text{ini}}\|P_{\text{fin}})}_{\textbf{complexity}},
%%     \label{eq:free}
%%   \end{equation}
%%   where
%%   \begin{equation}
%%     D_{\text{KL}}(P_{\text{fin}}\|P_{\text{ini}}) := \langle \log(\P_{\text{fin}}/\P_{\text{ini}})\rangle_{\P_{\text{fin}}} := \sum_{\omega \in \Omega}\P_{\text{fin}}(\omega)\log(\P_{\text{fin}}(\omega)/\P_{\text{ini}}(\omega))
%%   \end{equation}
%%   is the Kullback-Leibler divergence, and represents the information cost (measured in energy units) of changing the initial system.
%% In the above formula, we've extensively used the fact that $(U_{\text{ini}}, P_{\text{ini}})$ is a congugate pair and so $U_{\text{ini}}(\omega) \equiv \alpha \log(P_{\text{ini}}(\omega))$ by virtue of \eqref{eq:boltzmann}.
%%   The two terms in \eqref{eq:free} (accuracy or expected gain in utility, and the complexity of the transition) can be viewed as dertiminants of bounded rational decision-making. They formalize
%% a trade-off between an expected utility $\Delta U$ (first term) and
%% the information cost of transforming Pi
%% into $P_{\text{fin}}$ (second
%% term). In this interpretation $P_{\text{ini}}$ represents an initial choice
%% probability or policy, which includes the special case of the
%% uniform distribution where the decision-maker has initially no
%% preferences between the different choices. The probability measure $P_{\text{fin}}$ is the final choice probability that we are looking for since it
%% considers the utility constraint U∗
%% that we want to optimize. We can then formulate a variational principle for bounded rationality in the probabilities $P_{\text{fin}}(\omega)$
%% \begin{equation}
%% P^*_{\text{fin}} := \underset{P_{\text{fin}}}{\text{argmax }} \Delta J_{(U_{\text{ini}}, P_{\text{ini}}) \rightarrow P_{\text{fin}}}
%%   \end{equation}

%% By differentiating the RHS of \eqref{eq:free} w.r.t $P_{\text{fin}}$ and setting to zero, we obtain the closed-form solution

%% \begin{equation}
%%   P^*_{\text{fin}}(\omega) \propto P_{\text{ini}}(\omega)\exp\left(\frac{1}{\alpha}\Delta U(\omega)\right).
%% \end{equation}

%% Two limit cases are worth considering.
%% \begin{itemize}
%% \item \textbf{Low-temperature regime $\alpha \approx 0$:} Here $\Delta J_{(U_{\text{ini}}, P_{\text{ini}}) \rightarrow P_{\text{fin}}} \approx \langle \Delta U\rangle_{P'_{\text{fin}}}$, and so it's optimal to take
%%   $$P^*_{\text{fin}}(\omega) \equiv \text{dirac}(\omega - \omega^*) = \begin{cases}1, &\mbox{if }\omega = \omega^*,\\0, &\mbox{ otherwise,}\end{cases}$$
%%   where $\omega^* := \argmax_{\omega \in \Omega}U(\omega)$. This corresponds to unbounded rational decision-making, in which the cost of transition / problem-solving is completely disregarded.
%%   \item \textbf{High-temperature regime $\alpha \rightarrow +\infty$:} In this limiting case, it's optimal to take $P^*_{\text{fin}}(\omega) \equiv P_{\text{ini}}(\omega)$, i.e the change is so costly that it's optimal to maintain the current choice probabilities.
%% \end{itemize}

%% To conclude this section, let's note that \cite{braun2011path} show how their free-energy framework (on paths) links with the well-known Hamilton-Jacobi-Bellman optimal control framework. For example, one can re-derive the Linear Quadratic Gaussian (LQG) controller, which is a generalization of the LQR in \eqref{eq:hj}...

\section{Reinforcement Learning as a computational model for DMN function}
The main idea of this section is to argue that the DMN has biological substrates (gradient
computation, backprop, experience replay, etc.) for all the
major components of an efficient \textit{Reinforcement Learning} algorithm, and all these
components integrate in a plausible way.
Reinforcement Learning (RL) is learning to achieve a goal by interacting with an external
environment. In a general RL framework, an \textit{agent}
interacts with an  environment in a trial-by-error fashion by taking
actions / decisions $a$, which trigger changes in the
\textit{state} of the environment
$s \rightarrow s'$, accompanied by a real-valued \textit{reward}
$r = r(s, a)$ (or regret!, since it can be negative) collected by the
agent. In this view, the environmental is partially controlled by
the action of the agent.
This reward should be thought
of as an instantaneous satisfaction accompanying the execution of
an action.
%% Here the subscript $t$ denotes time, and can be discrete or
%% continuous.
The environment $\mathcal E$ is generally taken as stochastic,
is partially observed, in the sense that only part of the current state is observed by
the agent.
\subsection{Markov Decision Processes}
A popular abstract model for the scenario are so-called \textit{Markov Decision Processes (MDPs)}.
%% In RL, MDPs are the mathematical instantiation of the environment model,
%% which generally can incorporate action probabilities (policy matrix)
%% and/or value function given a state.
%% (Silver: The environment can be fully defined by state space and
%% transition probability matrix.)
is simply a quintuplet $(\mathcal S, \mathcal A, r, \rho, \mu)$ where
\begin{itemize}
\item $\mathcal S$ is the set of states, e.g $\mathcal S = \{\text{happy}, \text{sad}\}$.
\item $\rho : \mathcal S \times \mathcal A \times \mathcal S \rightarrow [0, 1],\; (s,a,s') \mapsto \rho(s'|s,a)$,
  the probability of moving to state $s'$ if action $a$ is taken from state $s$. In addition, one requires that such
  transitions be Markovian. Consequently, the future states are independent of past states and only depend on the present.  
\item $\mathcal A$ is the set of actions, e.g $\mathcal A = \{\text{read}, \text{run},
  \text{laugh}, \text{sympathize}, \text{empathize}\}.$
\item $r : \mathcal S \times \mathcal A \rightarrow \mathbb R$ is the \textit{reward function},
  so that $r(s, a)$ is the instant reward for taking action $a$ in state $s$.
\item $\mu: \mathcal S \rightarrow [0, 1]$ is the prior probability on the states so that
  $\mu(s)$ is the probability that the environment starts-off in state $s$.
\end{itemize}

\begin{remark}
  Given a system $(\mathcal S, \mathcal A, r, \rho, \mu)$ satisfying all the axioms for an MDP
  except the Markov property, we can always transform it into an MDP by considering a compounded
  version of the states
  $$S_t \leftarrow (S_0,A_0,R_0,S_1,A_1,S_2\ldots,S_t,A_{t-1},R_{t-1},S_{t})$$ made of the system's
  history up to an including time $t$.
\end{remark}

\paragraph{A note on model-based RL.}
Knowledge the agent has about the environment can be encoded in the
functions $r$, $\mu$, $\rho$, and / or $\mu$, as is done in
\textit{model-based} RL. For example if a rat in a maze knows, for sure,
that staying still will produce no change in the environment, and in particular will not eventually lead to finding the food, they can assume:
\begin{itemize}
\item $r(s, \text{``stand still''}) = 0$ if $s$ does not correspond to a cell / chamber containing food.
\item $\rho(s'|s,\text{``stand still''}) = 1$ if $s'=s$ and $0$ otherwise.
\item etc.
\end{itemize}

\subsection{Value functions and policies}
The behavior of the agent is governed by some kind of \textit{policy}, which maps states to
sampling probabilities over candidate actions to perform from this state. Starting a time $t=0$,
a policy $\pi$ generates a trajectory as follows
\begin{eqnarray*}
  \begin{split}
    S_1 &\sim \rho(.|S_0,A_0), \text{ collect rewards }R_0 = r(S_0, A_0)\\
    A_1 &\sim \pi(.|S_1)\\
    S_2 &\sim \rho(.|S_1,A_1), \text{ collect rewards }R_1 = r(S_1, A_1)\\
    \ldots\\
    A_{t} &\sim \pi(.|S_{t})\\
    S_{t+1} &\sim \rho(.|S_{t},A_{t}), \text{ collect rewards }R_{t} = r(S_{t}, A_{t})\\
    \ldots
  \end{split}
\end{eqnarray*}
Since an action taken now might have repercussions long into the feature, it turns out that the
quantity to optimize is not the instantaneous rewards $r(s, a)$, but a
cumulative version which takes into account the future. A standard for
modeling this accumulation is the so-called
time-discounted \textit{cumulative reward} function
\begin{equation}
  \label{eq:cumr}
  G^\pi = \sum_{t=0}^{\infty}\gamma^{t}R_t.
\end{equation}
This random variable\footnote{Random as it depends both on the environment's dynamic and the
  policy $\pi$ being played (which can be stochastic).}  measures the cumulative reward of
following a
policy $\pi$. The  goal of RL is then adapt this policy so as to  maximize the cumulative rewards.
In the definition of $G^\pi$ above, the constant $\gamma \in [0, 1]$ is the \textit{discount factor}. The choice $\gamma = 0$   corresponds to a perfectly myopic agent who is solely concerned about their immediate rewards and has no horizon
for the future. For there to be any learning at all, a technical requirement is that $\gamma < 1$.
% \footnote{In which case we can take $T \rightarrow +\infty$, provided the reward function $r$ is bounded.}.
Thus the agent is positively biased towards considering rewards which are will
come soon. The reward importance hence decays exponentially
with distance in the future,
given that there is more uncertainty in the farsighted future.
The reward discount factor $\gamma$ can thus be seen as calibrating
the delay discounting behavior of the intelligent agent.

The agent's goal can then be formulated as being that of finding policy $\pi$ which maximizes the
expected cumulative value of state-stat-pairs $(A_0,S_0)$, namely
\begin{equation}
  \label{eq:qval}
  Q^{\pi}(s,a) = \mathbb E [G^\pi|S_0=s,A_0=a,A_1 \sim \pi,A_2 \sim \pi,\ldots].
\end{equation}
That is, the expectated future reward over all possible trajectories in which we start at $s$ take
action $a$ and then follow the policy $\pi$ thereafter to select future actions.
%% Given that each $\pi$ correspondence to an exhaustive description of
%% behavioral patterns, $Q^{\pi}(S_t,A_t)$ corresponds to the epistemic value of
%% the set of actions given states in finite time.

By construction, in general, maximizing the expectation in  \eqref{eq:qval} is intractable due to
the enormous number of possible trajectories the agent might need to
consider.
(If the Bellman-based computational ideas have a corresponding
neurobiological implementation, this provides a framework of how
optimal behavior could be computable in the human brain.)
One popular simplification that removes this issue is to only consider the sub-family of
\textit{deterministic policies}, which map each state to the best single best action which
should be taken at thin state, and therefore define functions from states to actions.
\textit{Q-learning} (Watkins \& Dayan, 1992), a commonly used off-policy algorithm, uses the
\textit{greedy policy}
\begin{equation}
  \pi(s) = \argmax_{a \in \mathcal A}{Q}(s, a).
  \label{eq:qlearning}
\end{equation}
% where $\tilde{Q}(s, a)$ is the current estimate of the state-action value
% table.
The celebrated \textit{Bellman's equation} [Sutton and Barto 1998] then takes the simple form
\begin{equation}
  Q^\pi(s, a) = \mathbb E_{s' \sim \rho(.|s,a)} [r(s,a) + \gamma \max_{a' \in \mathcal A}Q^\pi(s', a')]
  = \mathbb E_{s' \sim \rho(.|s,a)} [r(s,a) + \gamma Q^\pi(s', \pi(s'))]
  \label{eq:bellman}
\end{equation}
which provides a recursive decomposition of optimal behavior.
A complicated dynamic programming optimization can be
broken into simpler subproblems at different time points.
Using the Bellman equation, each state can be associated with a certain value,
to guide action (i.e., improve the policy) towards a better state.
It is a fixed-point equation for the deterministic policy $\pi$.
Notice that in the expectation in \eqref{eq:bellman}, the sampling is done over only things which
depend on the environment, and so can be learned off-policy by observing state-transitions
triggered by another behavioral policy, which can be stochastic.

\subsection{Efficient RL with value approximation and replay}
%As already mentioned in the previous section, Q-learning optimizes over the class of
% deterministic policies of the form \eqref{eq:qlearning}.
We need ways to approximate the state-action value function
which scale up to large state spaces and / or action spaces.
In state $s \in \mathcal S$, the agent takes an action $a \in \mathcal A$
% (for example, $a$ = ``Read'', or ``Run'', or ``Laugh'', or ``Sympathize'', or ``Empathize'', etc.)
by sampling from its current policy matrix, and collects a reward $r$,
% (measured on their internal currency of worldly values ?),
and the environment transitions to a new state $s' \in \mathcal S$. At the end of
this step, a new \textit{experience} $e = (s,a,r,s')$ is produced; this represents an exemplar
behavior of the agent and is recorded in replay memory buffer:
$D \leftarrow D \cup \{e\}$ (possibly discarding the oldest entries to make space).
Now, at iteration $k+1$, replay consists in sampling (uniform or importance-weighted
\footnote{e.g weighted by TD error of the state transition $s \overset{a}{\rightarrow} s'$.} ?)  mini-batches of experiences
$(s, a, r, s') \sim \mathcal U(D)$ from the replay memory $\mathcal D$ and trying to
approximate
the would-be $Q$-value for the state-action pair $(s,a)$ as predicted by Bellman's equation \eqref{eq:bellman}, namely
$r + \gamma \max_{a'} Q(s', a'|\theta^Q_k)$, with the output of a parametrized regression model $(s,a,r,s')
\mapsto {Q}(s, a|\theta^Q_{k})$, where $Q(.,.|\theta^Q_{k})$ is an approximator for the $Q$-value operator,  parameterized by $\theta^Q_{k+1}$.
Computing an optimal policy then corresponds to finding the parameters $\theta^Q_{k+1}$ which minimize the following mean-squared loss function
\begin{equation}
  \mathcal L(\theta^Q_{k+1})
  % = \mathbb E_{(s, a, r, s') \sim \mathcal U(D)}[(Q(s, a|\theta^Q) - Q(s',a'|\theta^Q_{k+1}))^2]
  = \mathbb E_{(s, a, r, s') \sim \mathcal U(D)}\left[\frac{1}{2}(Q(s, a|\theta^Q_{k+1}) - y)^2\right],
  \label{eq:oracle}
\end{equation}
where
$ y = r + \gamma \max_{a' \in \mathcal A} Q(s', a'|\theta^Q_k) = r + \gamma Q(s', \pi(s')|\theta^Q_k)$ is the prediction target.
% While $y_t$ is also dependent on $\theta^Q$, this is typically ignored.
For example, a general linear model with a kernel $\phi$ would be of the
form
$${Q}(s, a|\theta^Q) = \phi(s,a)^T\theta^Q.$$
$\phi(s,a)$ would represent a high-level hand-crafted representation of the state-action $(s,a)$ into an appropriate
feature space. Such are the techniques that have been proposed by [sutton et al. xyzw, Song et al. 2016 (NIPS),
  and Parr et al. 2003 (JMLR)].
A breakthrough alternative proposed by Google DeepMind [Minh et al. 2014, Silver et al 2015] is to learn this
representation using a deep neural net, leading to the so-called \textit{Deep Q-learning} family of methods which
are by far the current state-of-the-art in RL. In such a case, $\theta^Q$
would correspond to ..

The entire model can then be effectively trained via \textit{backprop}
wherein online stochastic gradient descent is used to minimize the
variational objective \eqref{eq:oracle}.

\paragraph{Parameter update.}
\begin{equation}
  \nabla_{\theta^Q_{k+1}}\mathcal L(\theta^Q_{k+1})
  % = \mathbb E_{(s, a, r, s') \sim U(\mathcal D)}[(Q(s, a|\theta^Q) - Q(s',a'|\theta^Q_{k+1}))^2]
  = \mathbb E_{(s, a, r, s') \sim \mathcal U(D)}\left[\underbrace{(Q(s, a|\theta^Q_{k+1}) - y)}_{\text{regret}}
    \underbrace{\nabla_{\theta^Q_{k+1}}Q(s, a|\theta^Q_{k+1})}_{\text{???}}\right],
  \label{eq:oracle}
\end{equation}

\paragraph{Link to classical RL algorithms.}
Note that most RL algorithms, including \textit{Temporal Difference (TD)}
learning [Sutton et al., Bertsekas], REINFORCE [William], and SARSA can be
cast in this general variational framework. For example TD corresponds to
the above framework using a linear value approximator with feature encoding
$\phi(s,a) = \delta_{(s,a)} =  \text{ point mass at }(s,a)$ on the grid
$\mathcal S \times \mathcal A$, and so
$$\nabla_{\theta^Q_{k+1}}Q(s, a|\theta^Q_{k+1}) = \phi(s, a) = \delta_{(s,a)}.$$
%% Whence, the gradient update due to the sample $(s,a,r,s') \in \mathcal D$ is
%% $$\theta^Q(s,a) \leftarrow \theta^Q(s,a) + (Q(s, a|\theta^Q_{k+1}) - y)$$ ??

\subsection{Correspondence between DMN function and RL framework}
\textbf{XXX Put the diagrams and process charts here.}
\begin{itemize}
  \item The policy matrix represents the repertoire of possible actions
  on the world given a current state. It encodes the probabilites of
  choosing actions to be executed in a certain situation.
  \item Given a state, the value function represents the estimated utility of
  being in that state, which is subjective to the agent. 
  For a state $s$, the value function at $s$ namely $V(s)$ define provides a quantitative answer to the question:
  "How desirable is it to be in state $s$ ?"
  For each action $a$ that can be taken from state $s$,
  the Q-value  $Q(s, a)$ defined in \eqref{eq:qval} provides the subjective
  utility of executing a specific action; it answers the question
  "What is the expected utiltiy of taking action $a$ in this situation ?"
  \item The DMN may subserve
  constant exploration of possible future actions and their
  cumulative outcomes. Implicit computation of future choices 
  provides an explanation for purposeful mind-wandering.
  \item hippocampus -> provides perturbed action-transition-state-reward samples
  as a block of "imagined", "hypothesized", "recalled" experience;
  the small variations in these experience blocks allow searching
  a larger space of parameters and possible experiences.
  In the absence of environmental input and feedback
  (e.g., mind-wandering or sleep) pseudo-experiencing possible
  future scenarios and action outcomes.
  Our approach acknowledges the unavoidable stochasticity of
  computation in neuronal processes \cite{faisal2008noise}.
  \item
  \textit{Inference} in the human brain reduces to generalization of
  policy and value adaptations from sampled experiences to
  successful action choices and reward predictions in future states.
  \item In the RL view, plasticity in the DMN arises naturally.
  If an agent behaving optimally in a given environment moves
  to novel, yet unexperienced environments, reward prediction errors will
  massively increase. This will lead to adaptation of the policy matrix
  and value function until the system converges to a
  new steady-state of optimal action decisions given certain states.
  \item The DMN is today known to consistently increase in neural
  activity when engaging cognitive processes that are detached from
  current sensory environment. DMN involvement in thinking about the past,
  the future, and hypothetical possibilities ties in with the
  MDP's implicit computation of action and state cascades as a function
  of what happened in the past.
  
\end{itemize}





\paragraph*{Some notes:}
\begin{itemize}
  \item Actions are continuous, whilst states are continuous.
  \item The brain does backprop (see Hinton's recent talk at Stanford)
  \item model weights $\theta^Q$ should correspond to connections between neurons
\item humm, looks like such a knowledge would simply translate into constraints on the DqN weights $\theta^Q$ (... at least, part of such knowledge)
  ok, gotta think about the model-free / model-based reconsiliation
\end{itemize}


\section{Relation to other cognitive models (Danilo)}
\paragraph{Predictive Coding and Bayesian Brain Hypothesis}
The predictive coding and Bayesian brain framework 
is a frequently evoked idea related to the default mode function
\cite{bar2007}.
According to this view,
cortical responses emerge from continuous functional interaction between
higher and lower levels of the neural processing hierarchy.
This dynamic intercourse enables inference on the world by reconciling
gaps between fresh sensory input and stored prior information. 
Feed-forward sensory experience is constantly calibrated by
top-down modulation at various hierarchical processing levels.
back projections probably outnumber by far the existing input projection in the brain
\cite{salin1995corticocortical}.
It can explains the constructive, generative nature of sensory perception
\cite{friston2010free} and
why motor action is intimately related to sensory expectations
\cite{wolpert1995internal}.
At each stage of neural processing,
an internally generated prediction of the sensory input is
directly compared against the actual input from the environment.
A prediction error at one of the processing levels,
i.e. the difference between sensory input
and internally predicted sensation,
incurs plasticity changes of neuronal back-projections (i.e., model parameters)
for gradually improved future prediction of the environment.
Contextual integration is maintained by top-down modulation of sensorimotor
processing by context-specific a-priori information.
The generative model of how data arise in the
world hence be incorporated into
the current neuronal wiring.
Indeed,
This process permits updates of the internal model of the environment
to best fit the incoming sensory examples.


\begin{itemize}
  \item Both accounts are process theories backed up by plausible
  neurobiological evidence that
  view the brain as a \"statistical organ\"
  generalizing from the past to new experiences.
  \item Both expose a mechanisms of brain plasticity with increased
  adaption of neuronal wiring when faced by unprecedented environmental
  challenges.
  \item The hierarchical aspects from predictive coding
  is re-expressed in MDP in form of
  nested prediction of probable upcoming actions and rewards.
  \item Both model the consequences of action. In RL, the horizon of that
  look into the future is manifest in the $\gamma$ parameter
  in the Bellman equation, but not
  explicitly modeled in the PC account that
  mostly emphasizes prediction about the
  immediate future.
  \item Mismatch negativity of PC is immanent in reward prediction error
  by the difference between actually predicted reward of an action given
  a state and the reward predicted by non-linear regression with graded
  discounting of future rewards.
  \item Sensory experience is a generative process from both views.
  In predictive coding, sensory perception of the external world
  is a generative experience due to modulatory top down experience at
  various hierarchical levels of sensory input processing.
  In our RL view, the (partially observed)
  environmental model is incorporated in the Markov decision
  process, \textit{which can be fully recovered based on the last
  state alone}.
  \item Both provide a parsimoneous explanation why
  humans seem to minimize processing of incoming information
  when the environment is predictable and allocate attention when
  novel stimuli are encountered.
  Similarly, both propose explanations why
  predicting the future is inherently
  linked to information from the past.
\end{itemize}



\paragraph{Semantic Hypothesis}

The proposal promoted here is that the brain is continually engaged in generating predictions, and that these predictions rely on associative activation. 
what people do when their mental capacity is not completely consumed by a specific task is to generate associations
Traditionally this process has been considered as recognition, classification, or even a type of memory retrie- val, but in the present context I treat this process as analogy- making instead. 
in analogy, the emphasis is on ‘what is it like?
Analogy- based mappings of properties manifest themselves in processes ranging from perception and memory [17] to stereotypic judgments and prejudice
While our existing memories are used to derive analo- gies and activate predictions, they are constantly being updated. The analogical process, in addition to affording the interpretation of our environment, subsequently aug- ments previous representations in a way that fosters increasingly flexible future analogies.
(Bar)

phantastic organ (from Greek phantastikos, the ability to create mental images)
Friston2014

Navigation processes require associative processing to link land- marks with spatial locations, episodic memories require associa- tions for binding the constituents of an episode, emotional processing and the concomitant heightened arousal they entail are tightly associated with affective memories, and our percep- tion of ourselves and others require associations formed with experience to help interpret the present and anticipate the future.
In some respects, predictions may be considered a higher level of thought-element than associations; they also provide the basis to the many processes detailed above as occupying the unconstrained mind (e.g., planning, navigating, anticipating the thoughts and actions of others, stereotyping, imagining our- selves in the future, affective forecasting, etc.). The brain seems to be able to extract regularities and familiar patterns from our environment, and associate them with corresponding knowl- edge in memory to afford the generation of predictions. This idea of predictions as the basis of many, if not most, cognitive processes in the human brain will have to be developed sepa- rately because the observations we described here cannot speak to it directly
Bar2007

as showing that animals could learn a “cognitive map of the environment” in the absence of rewards or penalties, and that they could use the map later when they were motivated to reach a goal (Tolman, 1948). 

this perspective can be traced back to Hermann von Helmholtz and the notion of unconscious inference: that is, a pre-rational mechanism by which visual impressions are formed (eg, the seemingly automatic but erroneous belief that the sun rises and sets in the sky, as opposed to the truth that the Earth rotates around it).

mem- ory-based predictions, and these predictions are generated continually either based on gist information gleaned from the senses or driven by thought. The emphasis in this.
The proposed account integrates three primary components. The first is associations, which are formed by a lifetime of extracting repeating patterns and statisti- cal regularities from our environment, and storing them in memory. The second is the concept of analogies, whereby
Corresponding author: Bar, M. (bar@nmr.mgh.harvard.edu). Available online 4 June 2007.
we seek correspondence between a novel input and existing representations in memory (e.g. ‘what does this look like?’). Finally, these analogies activate associated representa- tions that translate into predictions 
(Bar)


A second important functional account of the DMN is the 'semantic hypothesis'. It holds that retrieval of building blocks of world knowledge enables language and other high-level faculties. The DMN central involvement in semantic processing results in its implication in many a-priori unrelated aspects of human behavior, such as reasoning, planning, problem solving and decision making (Binder, Desai, Graves, \& Conant, 2009; Schilbach et al., 2008; Suddendorf \& Corballis, 2007). This is also illustrated by the substantial relationship between semantic knowledge and words (Binder et al., 2009). The engagement of DMN in language processing matches the observation that the large majority of the DMN regions such as TPJ receive no direct input from primary sensory areas acting more as a “hermeneutic hub” (Binder et al., 2009; Seghier, 2013). The DMN activation rather enables the organization and processing of internally generated information in absence of stimulus presentation (Binder et al., 1999). The proposed role of the DMN in semantic processing is further highlighted by the convergence of these regions in a reciprocal relationship as an interface between the allocentric (bird’s eye view) and egocentric (first person view) perspective; in this way self-reflection, social knowledge, and autobiographical memories are integrated. Moreover, the semantic account exquisitely explains the engagement of DMN in both recalling past and envisioning future (Demblon, Bahri, \& D'Argembeau, 2016; Hassabis, Kumaran, Vann, \& Maguire, 2007; Schacter et al., 2007; Spreng, Mar, \& Kim, 2009), in that the internally stored in language world knowledge may lay the contextual foundations for envisioning future events as well as memory processes (Bzdok et al., 2012). Hence, the reported retrograde amnesia and the simultaneous impairment of imagining novel experience across patients with hippocampus lesions ties in well with this model (Hassabis \& Maguire, 2007). In other words, DMN activation might underlie a time dimension independent cognitive process of constructing contextual scenarios out of detached memories (Buckner \& Carroll, 2007; Gaesser, Spreng, McLelland, Addis, \& Schacter, 2013; Hassabis \& Maguire, 2007). Summing up the 'semantic account', DMN offers the conceptual framework that underlies not only language but also other major high-level functions of human behavior.

\begin{itemize}
  \item The notion of semantic or knowledge association is
  incorporated into the MDP as a function of the state and action
  history.
  \item Both: there is no evidence to indicate that predictions of various levels of complexity, abstraction, timescale and purpose use mech- anisms that are qualitatively different
\end{itemize}


\paragraph{Sentinel Hypothesis}
many investigators speculate that this central neural network reflects the brain’s relentless tracking and prediction of environmental events to adaptively optimize the organism's future action (Schacter et al., 2007).

Cognitive neuroimaging studies linked the DMN to the contemplation of others’ (Mar, 2011) and one’s thoughts (Lombardo et al., 2009) mental states, 

DMN does social cognition -> but it exists in rats and monkeys with much more poorly developped social capacities
-> our computational framework can be equally applied to humans
and other primates.

How can a same neurobiological circuit be equally important for baseline house-keeping functions and specific task performance?

Humans have learned to solve ecological problems
by social means.

Our computational account can explain why the DMN is implicated in both a goal-directed task and an idlying rest cognitive set? -> task is policy/value updates to optmize short-term action / rest updates for mid- and long-term action


 The 'sentinel hypothesis' is a major functional account to explain DMN activity that this canonical network acts as a cohesive unit. This coherent neurobiological network would be prominently engaged as a “radar” as well as in self-referential information processing including mind wandering, autobiographical recall, mentalizing, reflection about past and future (Buckner, Andrews-Hanna, \& Schacter, 2008; Schilbach, Eickhoff, Rotarska-Jagiela, Fink, \& Vogeley, 2008). Acting as a sentinel, DMN functions as a broad focus manager that performs continuous environmental tracking for salient and unexpected events (Buckner et al., 2008). This default activity may allow the detection of possible predators and potentially serves an evolutionarily conserved purpose by means of attention allocation (Raichle et al., 2001). The “radar” function may be efficiently supported by the activation patterns of DMN in diffuse attention condition tasks with targets appearing in various locations (Hahn, Ross, \& Stein, 2007). On the other hand, the salient information perceived from the external environment activates self-referential information processing (Krienen, Tu, \& Buckner, 2010). A conceptualization of these functions suggests two highly adaptive interacting DMN subsystems (Andrews-Hanna, Saxe, \& Yarkoni, 2014; Schacter, Addis, \& Buckner, 2007). However, the sentinel hypothesis may not exhaustively explain the variety of findings on the DMN connectivity with memory-related structures such as hippocampus (Giugni, Vadala, De Vincentiis, Colica, \& Bastianello, 2010; McCormick et al., 2014). Summing up this first candidate account for DMN function, the sentinel hypothesis heavily emphasizes on the DMN as a single structure with a continuous sentinel role increasing during self-referential tasks.




\section{Relation to other statistical models (Elvis)}
\paragraph{Control theory.}
\begin{itemize}
  \item The model proposed in \cite{betzel2016} can be cast in the form \eqref{eq:hj}, with the particular choice $\Q = \textbf{I} = \rho^{-1}\R$ and $\Q_{\text{fin}} = \textbf{0}$, but with a rather ad-hoc handle on stability issues and choice driver nodes...
\end{itemize}

\paragraph{Reinforcement Learning.}
abc

\paragraph{Free Energy Principle.}
\begin{itemize}
  \item Closely related to Friston's free-energy principle: brain as inference engine
  biological systems, including brains, must minimize the long-term average of surprise;
  \item A common point between the Free Energy principle and our framework
  is placing the minimization of suprise at the core of brain function.
  In RL, surprise minimization reduces to accurate prediction of 
  rewarding outcomes in the future.
  \item Ortega et al. 2013 \cite{ortega2013thermodynamics} ``Thermodynamics as a theory of decision-making with information-processing costs''
\item Friston's critic of RL \cite{fristonAIorRL} (he proposes "active inference''): "This equation (equation 18) comes from the theory of dynamic programming,
pioneered in the 1950s by Richard Bellman and colleagues [2]. To
optimise control a~pð Þ x~ under this formulation, we have to: (i)
assume the hidden states are available to the agent and (ii) solve
Equation 18 for the value-function. Solving for the value-function
is a non-trivial problem and usually involves backwards induction
or some approximation scheme like reinforcement learning [4–6].
The free-energy formulation circumvents these problems by
prescribing the policy in terms of free-energy, which encodes
optimal control (Equation 6)"
\item It is difficult to compute the free energy principle
in practice, which makes it physiologically less plausible.
  \end{itemize}

\section{Conclusion}
What single brain function could be most important for existence and survival of the species?
Estimating quantities that depend on how features of an agent’s environment are expected to unfold over the future.
From the perspective of the human condition, it remains unclear what the nature and purpose of a domain-overarching computational mechanism implemented in the high association cortex could be to warriant the high energetic costs.
MDP conceptualize control of the environment.
Obtain reward and avoid pain by adaptive behavior.

The idealized computational framework explains previous observations about the
DMN in a simple in conception but non-trivially links
together disparate functional hypotheses in the DMN literature.
The default brain function may be reducible to a small number
of well-defined equations.
biological plausibility of dynamic optimization in
optimal substructure
mathematical formalism; statistical solution; adaptive fitness
; formalize and predict; offer statistical control on the brain's default function/operations
stochastic modelling
At the least, we propose an alternative vocabulary to
describe and discuss neuroscientific studies on the DMN.
%
In principle,
neuroscientific experiments can be designed that operationalize
the set of action, value, and state variables that determine
the behavior of RL systems. This makes proposed machine-learning
approach to DMN function not only practically computable but
also neuroscientifically falsifiable.
%
The DMN can be viewed as a smoothing kernel extending
from the relatively recent past to the relatively close future
that constantly estimates complex non-linear gain functions.
-> This is why neuroimaging research revealed activity increases in the DMN
when 
-> This may be why perfect memory has been evolutionarily de-selected,
although it is provably feasible by the neurobiological hardware in humans




\paragraph{Acknowledgment}
% {\small The research leading to these results has received funding from the
% European Union Seventh Framework Programme (FP7/2007-2013)
% under grant agreement no. 604102 (Human Brain Project).
% Further support was received from
% the German National Academic Foundation (D.B.).
% }


\small
\bibliographystyle{splncs03}
\bibliography{refs}

\end{document}
